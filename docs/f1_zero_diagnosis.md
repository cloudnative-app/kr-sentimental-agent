# F1이 0으로 나오는 이유 — 진단 요약

## 골드 데이터셋 통계 (mini3 valid.gold.jsonl)

| 항목 | 값 |
|------|-----|
| **총 행 수** | 30 |
| **골드 속성(gold_tuples)이 아예 없는 행** | **0개** (모든 행에 1개 이상 gold_tuples 존재) |
| **총 골드 튜플 수** | 31 (29행×1개 + **1행×2개**) |
| **aspect_term이 비어 있는 튜플** | 17개 |
| **aspect_term이 있는 튜플** | 14개 |

→ 한 **행(uid)**에 `gold_tuples` **배열**이 하나 있고, 그 안에 튜플이 1개 또는 여러 개 들어갈 수 있습니다. 6번째 행(`nikluge-sa-2022-train-00407`)만 튜플이 2개(본품#품질·아연합금 재질, 제품 전체#디자인)라서 30행인데 튜플 합계는 31개입니다.  
→ “속성이 없는 데이터”는 **0건**입니다. 다만 **17개 튜플**은 `aspect_term`이 빈 문자열(`""`)이라, 정규화 시 `aspect_ref`(택소노미 라벨)로 채워져 골드 페어가 `(본품#품질, positive)` 등이 됩니다. 예측은 문장 span이라 `(주머니, positive)` 등이 되어 **매칭되지 않습니다**.

## 채점 방식 (현행)

- **평가 정의**: tuple set 채점에 사용하는 기준은 **aspect_term**(문장 내 표면형)과 **polarity**뿐임. **aspect_ref는 채점 로직에서 사용하지 않음.**
- **매칭 기준**: **(aspect_term, polarity)** 쌍만 사용. aspect_ref는 **무시**.
- **대상**: 골드가 있는 샘플만 F1 집계에 포함; 골드가 없는 샘플은 제외.
- 추후 정책에서 데이터 정제 후 의미 있는 F1 기대.

## (참고) 이전 현상 및 원인

- `N_gold=30`으로 골드가 로드되어 있어도, **(aspect_ref, aspect_term, polarity)** 3항 완전 일치 기준에서는 `tuple_f1_s1`, `tuple_f1_s2`, `delta_f1` 등이 **0.0**으로 보고됨.

**원인**: aspect_ref 표현 불일치. 이전에는 **(aspect_ref, aspect_term, polarity)** 튜플 단위로 **완전 일치**할 때만 정답 처리했음.

| 구분 | aspect_ref 예시 | 출처 |
|------|------------------|------|
| **골드** | `본품#품질`, `제품 전체#일반`, `제품 전체#디자인`, `패키지/구성품#편의성` | NIKL taxonomy (`valid.gold.jsonl` → `inputs.gold_tuples`) |
| **예측** | `주머니`, `몸`, `철픽`, `용물`, `몸썻캐과`, `용물철픽캐캐출 캐름` 등 | 파이프라인 ATSA/Moderator 출력 (문장 내 **span 텍스트**) |

- 골드는 **데이터셋 레이블**(NIKL 대분류#소분류)을 사용하고,
- 모델/파이프라인은 **문장에서 추출한 구체 표현(span)**을 `aspect_ref`로 내보냅니다.

두 체계가 다르기 때문에 정규화 후에도 **한 건도 일치하지 않고**, F1이 0이 됩니다.

## 검증 방법

1. **골드 1건**  
   `experiments/configs/datasets/mini3/valid.gold.jsonl` 첫 줄:
   - `aspect_ref`: `"본품#품질"`, `aspect_term`: `""`, `polarity`: `"positive"`  
   → 평가 시 tuple: `(본품#품질, 본품#품질, positive)` (aspect_term 없을 때 aspect_ref로 채움).

2. **동일 uid scorecard의 예측**  
   `inputs.aspect_sentiments` / `runtime.parsed_output.final_result.final_aspects`:
   - `aspect_ref`: `"주머니"`, `"몸"`, `"철픽"`, `"용물"`, `"몸썻캐과"`, `"용물철픽캐캐출 캐름"` 등 (실제로는 한글 span이 인코딩 이슈로 깨져 보일 수 있음).  
   → tuple: `(주머니, …)`, `(몸, …)` 등.

3. **매칭**  
   `(본품#품질, 본품#품질, positive)` 와 `(주머니, …, positive)` 등은 **전혀 다른 aspect_ref**이므로 매칭 0건 → F1 = 0.

## 결론

- **채점/집계 로직 오류가 아니라**,  
  **“골드 표현(택소노미)”과 “모델 출력(span)”이 서로 다른 체계**라서 발생하는 **설계/데이터 불일치**입니다.
- F1을 의미 있게 쓰려면 아래 중 하나(또는 조합)가 필요합니다.

## 권장 대응

1. **택소노미 매핑 도입**  
   - 학습/추론 단계에서 **span → NIKL taxonomy** 매핑을 한 뒤,  
     예측 tuple의 `aspect_ref`(및 필요 시 `aspect_term`)를 골드와 같은 체계로 바꾸고 평가.  
   - 또는 평가 전용 **매핑 테이블/모듈**을 두고, 예측 aspect_ref만 taxonomy로 치환한 뒤 기존 tuple F1 계산 사용.

2. **골드를 span 기준으로 변경**  
   - 태스크가 “문장 내 span 단위 ABSA”라면,  
     골드도 `aspect_ref`/`aspect_term`을 **문장 내 표면형(span)**으로 두고,  
     현재 파이프라인 출력과 같은 형식으로 맞추기.  
   - 이 경우 `valid.gold.jsonl` 등 골드 생성 방식 수정 필요.

3. **평가 정의 명시**  
   - 논문/보고서에 “F1은 **(택소노미 기준)** aspect_ref 일치 기준”인지,  
     “**(span 기준)** 표면형 일치 기준”인지 명시하고,  
     골드와 예측이 **같은 정의**를 쓰도록 파이프라인/데이터를 맞추기.

위 중 어떤 정의를 채택할지 정한 뒤, 골드 형식 또는 예측→골드 매핑을 그에 맞추면 F1이 0이 아닌 값으로 의미 있게 집계됩니다.

---

## 채점이 잘 안 되는 다른 이유

1. **골드 aspect_term 빈 값 → 택소노미로 채움**  
   골드에서 `aspect_term`이 `""`이면 코드에서 `aspect_ref`로 채웁니다. 그래서 골드 페어가 `(본품#품질, positive)`처럼 **택소노미 문자열**이 되고, 예측은 `(주머니, positive)`처럼 **span**이라 (aspect_term, polarity)만 봐도 **일치하지 않습니다**.  
   → 31개 튜플 중 17개가 이 경우라, 채점에 유리한 “표면형이 있는” 골드는 14개뿐입니다.

2. **aspect_term 표기·공백 차이**  
   정규화는 소문자·공백 축소·앞뒤 구두점 제거만 합니다.  
   - 골드 `"뿌리는 마스크팩"` vs 예측 `"뿌리는마스크팩"` → 서로 다른 문자열로 남아 **매칭 실패**할 수 있습니다.  
   → 골드/예측 표기 통일(공백 정책, 띄어쓰기 정규화)이 필요할 수 있습니다.

3. **골드가 긴 구문/제품명인 경우**  
   골드 aspect_term이 `"판테놀, 프로폴리스 추출물, 마데카소사이드 주요성분 3가지"`처럼 길면, 모델이 같은 문자열을 그대로 뽑지 않아 **정확 일치가 거의 나오지 않습니다**.  
   → 표면형 기준 채점이라면, 골드를 짧은 span/동의 표현으로 정제하거나, 부분 일치/매핑 규칙을 도입하는 편이 좋습니다.

4. **polarity 표기**  
   현재 `normalize_for_eval`은 `pos`/`positive`, `neg`/`negative`를 별도 문자열로 취급합니다.  
   골드는 `"positive"` 등 풀네임, 예측도 동일하면 문제 없으나, 일부만 `"pos"`를 쓰면 **불일치**가 납니다.  
   → 필요 시 polarity 정규화(예: pos→positive, neg→negative, neu→neutral)를 추가하는 것이 안전합니다.
